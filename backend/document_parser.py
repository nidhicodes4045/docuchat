# -*- coding: utf-8 -*-
"""Document_Parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvf4_iwEiRuHA0HQPouVPwJ6A8KN7Cml
"""

#Basic imports
import os
from dotenv import load_dotenv
import langchain

#Langchain imports
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS as FAISS
from faiss import IndexFlatL2
from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.prompts import PromptTemplate

#Load .env and set up keys
load_dotenv()

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

#Initialise requirements

#Embeddings
embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)

#Initialise FAISS DB (1536 dimension index)
faissdb = FAISS(
    embedding_function=embeddings,
    index=IndexFlatL2(1536),
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
    normalize_L2=True
)

#Define a function to initialise the correct loader based on the number of files.
#Chunk and store the embeddings in the FAISS Database

def load_embed_files(filelist):
  if len(filelist) == 1:
    doc_loader = PyPDFLoader(filelist[0])
  else :
    folder_name = os.path.dirname(filelist[0])
    doc_loader = PyPDFDirectoryLoader(folder_name)

  docs = doc_loader.load()

  #Split and embed
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)
  split_text = text_splitter.split_documents(docs)
  faissdb.add_documents(split_text)

#Initialise openai
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)

#Initialise retriever
retriever = faissdb.as_retriever(search_kwargs={"k": 5})

#Define prompt and prompt templates
prompt_template = """ You are a helpful document assistant. Use the following context from a document to answer the question.
  If the answer is not contained in the context, say "I don't have enough information."
  Context: {context} Question: {query}"""
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template
    )

#Function to take in user query and get relevant chunks
def query_ans (query : str):
  relevant_chunks = retriever.get_relevant_documents(query)
  context = "\n\n".join([doc.page_content for doc in relevant_chunks])
  prompt_final = prompt.format(context, query)
  response = llm.invoke(prompt)
  return response.content